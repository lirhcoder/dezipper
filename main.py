#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡è§£å‹ç¼©å·¥å…· - ä¸»ç¨‹åº
"""

import os
import sys
import shutil
import time
import logging
import argparse
from pathlib import Path
from datetime import datetime

from config import (
    SUPPORTED_EXTENSIONS, MAX_PROCESSING_ROUNDS, 
    LOG_FORMAT, LOG_FILENAME_FORMAT, STATS_FIELDS
)
from utils import (
    format_file_size, get_file_extension, 
    get_unique_backup_name, ensure_directory_exists
)
from extractors import get_extractor


class BatchExtractor:
    """æ‰¹é‡è§£å‹ç¼©å·¥å…·ä¸»ç±»"""
    
    def __init__(self, work_dir, create_backup=True, delete_original=True, 
                 preserve_structure=True, extract_flat=False):
        self.work_dir = Path(work_dir)
        self.create_backup = create_backup
        self.delete_original = delete_original
        self.preserve_structure = preserve_structure
        self.extract_flat = extract_flat
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.stats = {field: 0 for field in STATS_FIELDS}
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_filename = datetime.now().strftime(LOG_FILENAME_FORMAT)
        log_path = self.work_dir / log_filename
        
        logging.basicConfig(
            level=logging.INFO,
            format=LOG_FORMAT,
            handlers=[
                logging.FileHandler(log_path, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def log_info(self, message):
        """è®°å½•ä¿¡æ¯æ—¥å¿—"""
        self.logger.info(message)
        
    def log_error(self, message):
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        self.logger.error(message)
        
    def log_success(self, message):
        """è®°å½•æˆåŠŸæ—¥å¿—"""
        self.logger.info(f"âœ… {message}")
        
    def log_warning(self, message):
        """è®°å½•è­¦å‘Šæ—¥å¿—"""
        self.logger.warning(f"âš ï¸ {message}")

    def create_backup_copy(self):
        """åˆ›å»ºå¤‡ä»½å‰¯æœ¬"""
        if not self.create_backup:
            self.log_info("è·³è¿‡å¤‡ä»½åˆ›å»º")
            return True
            
        backup_path = get_unique_backup_name(self.work_dir)
        
        self.log_info(f"ğŸ“‹ å¼€å§‹åˆ›å»ºå¤‡ä»½å‰¯æœ¬...")
        self.log_info(f"ğŸ“‹ åŸå§‹ç›®å½•: {self.work_dir}")
        self.log_info(f"ğŸ“‹ å¤‡ä»½ç›®å½•: {backup_path}")
        
        try:
            shutil.copytree(self.work_dir, backup_path)
            self.log_success(f"å¤‡ä»½åˆ›å»ºå®Œæˆ: {backup_path}")
            return True
        except Exception as e:
            self.log_error(f"å¤‡ä»½åˆ›å»ºå¤±è´¥: {str(e)}")
            return False

    def scan_compressed_files(self):
        """åˆå§‹æ‰«ææ‰€æœ‰å‹ç¼©æ–‡ä»¶ï¼ˆä»…ç”¨äºç»Ÿè®¡ï¼‰"""
        self.log_info("ğŸ” å¼€å§‹åˆå§‹æ‰«æå‹ç¼©æ–‡ä»¶...")
        
        compressed_files = []
        total_size = 0
        
        for root, dirs, files in os.walk(self.work_dir):
            for file in files:
                file_path = Path(root) / file
                file_ext = get_file_extension(file_path)
                
                if file_ext in SUPPORTED_EXTENSIONS:
                    file_size = file_path.stat().st_size
                    compressed_files.append((file_path, file_size))
                    total_size += file_size
                    
                    relative_path = file_path.relative_to(self.work_dir)
                    self.log_info(f"ğŸ“¦ å‘ç°å‹ç¼©æ–‡ä»¶: {relative_path} ({format_file_size(file_size)})")
        
        self.log_info(f"ğŸ“Š åˆå§‹æ‰«æç»“æœ:")
        self.log_info(f"   ğŸ“¦ å‹ç¼©æ–‡ä»¶æ€»æ•°: {len(compressed_files)} ä¸ª")
        self.log_info(f"   ğŸ“ å‹ç¼©æ–‡ä»¶æ€»å¤§å°: {format_file_size(total_size)}")
        
        if not compressed_files:
            self.log_warning("æœªå‘ç°ä»»ä½•æ”¯æŒçš„å‹ç¼©æ–‡ä»¶")
            self.log_info(f"æ”¯æŒçš„æ ¼å¼: {', '.join(SUPPORTED_EXTENSIONS.keys())}")
        else:
            self.log_info("âš ï¸ æ³¨æ„ï¼šç”±äºæ”¯æŒåµŒå¥—è§£å‹ï¼Œå®é™…å¤„ç†çš„æ–‡ä»¶æ•°é‡å¯èƒ½æ›´å¤š")
            
        return compressed_files

    def scan_compressed_files_current_round(self):
        """æ‰«æå½“å‰è½®æ¬¡çš„å‹ç¼©æ–‡ä»¶"""
        compressed_files = []
        
        for root, dirs, files in os.walk(self.work_dir):
            for file in files:
                file_path = Path(root) / file
                file_ext = get_file_extension(file_path)
                
                if file_ext in SUPPORTED_EXTENSIONS:
                    file_size = file_path.stat().st_size
                    compressed_files.append((file_path, file_size))
                    
                    relative_path = file_path.relative_to(self.work_dir)
                    self.log_info(f"   ğŸ“¦ å‘ç°: {relative_path} ({format_file_size(file_size)})")
        
        return compressed_files

    def get_extraction_path(self, archive_path):
        """è·å–è§£å‹ç›®æ ‡è·¯å¾„"""
        if self.preserve_structure:
            # åœ¨åŸä½ç½®åˆ›å»ºåŒåæ–‡ä»¶å¤¹
            return archive_path.parent / archive_path.stem
        else:
            # åœ¨å·¥ä½œç›®å½•æ ¹éƒ¨åˆ›å»ºæ–‡ä»¶å¤¹
            return self.work_dir / archive_path.stem

    def extract_single_file(self, archive_path, archive_size):
        """è§£å‹å•ä¸ªæ–‡ä»¶å¹¶ç¡®ä¿åˆ é™¤åŸæ–‡ä»¶"""
        relative_path = archive_path.relative_to(self.work_dir)
        extract_to = self.get_extraction_path(archive_path)
        file_ext = get_file_extension(archive_path)
        
        self.log_info(f"ğŸ”„ [{self.stats['processed'] + 1}] æ­£åœ¨å¤„ç†: {relative_path}")
        self.log_info(f"   ğŸ“ æ–‡ä»¶å¤§å°: {format_file_size(archive_size)}")
        self.log_info(f"   ğŸ“‚ ç›®æ ‡è·¯å¾„: {extract_to.relative_to(self.work_dir)}")
        
        extraction_success = False
        extracted_count = 0
        
        try:
            # åˆ›å»ºç›®æ ‡ç›®å½•
            ensure_directory_exists(extract_to)
            
            # è·å–å¯¹åº”çš„è§£å‹å™¨
            extractor = get_extractor(file_ext, self.logger)
            extracted_count = extractor.extract(archive_path, extract_to, self.extract_flat)
            
            extraction_success = True
            self.log_success(f"è§£å‹æˆåŠŸï¼Œæå–äº† {extracted_count} ä¸ªæ–‡ä»¶/æ–‡ä»¶å¤¹")
            self.stats['success'] += 1
            self.stats['extracted_files'] += extracted_count
            
        except Exception as e:
            self.log_error(f"   âŒ è§£å‹å¤±è´¥: {str(e)}")
            self.stats['error'] += 1
            
        finally:
            self.stats['processed'] += 1
            
            # åªæœ‰åœ¨è§£å‹æˆåŠŸä¸”ç”¨æˆ·è¦æ±‚åˆ é™¤åŸæ–‡ä»¶æ—¶æ‰åˆ é™¤
            if extraction_success and self.delete_original:
                try:
                    # ç¡®ä¿æ–‡ä»¶å­˜åœ¨åå†åˆ é™¤
                    if archive_path.exists():
                        archive_path.unlink()
                        self.stats['freed_size'] += archive_size
                        self.log_info(f"   ğŸ—‘ï¸ å·²åˆ é™¤åŸå‹ç¼©æ–‡ä»¶: {relative_path}")
                    else:
                        self.log_warning(f"   âš ï¸ åŸæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ é™¤: {relative_path}")
                except Exception as delete_error:
                    self.log_error(f"   âŒ åˆ é™¤åŸæ–‡ä»¶å¤±è´¥: {str(delete_error)}")
                    # åˆ é™¤å¤±è´¥ä¸å½±å“æ•´ä½“æˆåŠŸçŠ¶æ€
            
            elif extraction_success and not self.delete_original:
                self.log_info(f"   ğŸ“¦ ä¿ç•™åŸå‹ç¼©æ–‡ä»¶: {relative_path}")
        
        return extraction_success

    def process_all_files(self):
        """å¤„ç†æ‰€æœ‰å‹ç¼©æ–‡ä»¶ï¼ˆæ”¯æŒåµŒå¥—è§£å‹ï¼‰"""
        self.log_info("=" * 60)
        self.log_info("ğŸš€ å¼€å§‹æ‰¹é‡è§£å‹å¤„ç†ï¼ˆæ”¯æŒåµŒå¥—è§£å‹ï¼‰...")
        
        start_time = time.time()
        round_count = 0
        
        while True:
            round_count += 1
            self.log_info(f"\nğŸ”„ ç¬¬ {round_count} è½®æ‰«æå’Œè§£å‹...")
            
            compressed_files = self.scan_compressed_files_current_round()
            
            if not compressed_files:
                self.log_info("âœ… æ²¡æœ‰å‘ç°æ›´å¤šå‹ç¼©æ–‡ä»¶ï¼Œå¤„ç†å®Œæˆ")
                break
            
            self.log_info(f"ğŸ“¦ æœ¬è½®å‘ç° {len(compressed_files)} ä¸ªå‹ç¼©æ–‡ä»¶")
            
            # è®°å½•æœ¬è½®å¤„ç†å‰çš„ç»Ÿè®¡
            round_start_success = self.stats['success']
            round_start_error = self.stats['error']
            
            for i, (archive_path, archive_size) in enumerate(compressed_files):
                self.extract_single_file(archive_path, archive_size)
                
                # æ·»åŠ åˆ†éš”çº¿
                if i < len(compressed_files) - 1:
                    self.log_info("â”€" * 30)
            
            # æœ¬è½®ç»Ÿè®¡
            round_success = self.stats['success'] - round_start_success
            round_error = self.stats['error'] - round_start_error
            
            self.log_info(f"ğŸ¯ ç¬¬ {round_count} è½®å®Œæˆ: æˆåŠŸ {round_success} ä¸ª, å¤±è´¥ {round_error} ä¸ª")
            
            # é˜²æ­¢æ— é™å¾ªç¯ï¼ˆå¦‚æœæœ¬è½®æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶ï¼‰
            if round_success == 0:
                self.log_warning("æœ¬è½®æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶ï¼Œåœæ­¢ç»§ç»­æ‰«æ")
                break
                
            # é™åˆ¶æœ€å¤§è½®æ•°ï¼Œé˜²æ­¢å¼‚å¸¸æƒ…å†µ
            if round_count >= MAX_PROCESSING_ROUNDS:
                self.log_warning(f"å·²è¾¾åˆ°æœ€å¤§å¤„ç†è½®æ•°({MAX_PROCESSING_ROUNDS})ï¼Œåœæ­¢å¤„ç†")
                break
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        self.log_info("=" * 60)
        self.log_success("æ‰€æœ‰åµŒå¥—è§£å‹å¤„ç†å®Œæˆï¼")
        self.log_info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        self.log_info(f"   ğŸ”„ å¤„ç†è½®æ•°: {round_count} è½®")
        self.log_info(f"   ğŸ“¦ æ€»å¤„ç†å‹ç¼©åŒ…: {self.stats['processed']} ä¸ª")
        self.log_info(f"   ğŸ“„ æ€»æå–æ–‡ä»¶: {self.stats['extracted_files']} ä¸ª")
        self.log_info(f"   âœ… æˆåŠŸè§£å‹: {self.stats['success']} ä¸ª")
        self.log_info(f"   âŒ å¤„ç†å¤±è´¥: {self.stats['error']} ä¸ª")
        self.log_info(f"   â±ï¸ æ€»è€—æ—¶: {processing_time:.2f} ç§’")
        
        if self.delete_original and self.stats['freed_size'] > 0:
            self.log_info(f"   ğŸ’¾ é‡Šæ”¾ç©ºé—´: {format_file_size(self.stats['freed_size'])}")

    def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        self.log_info("ğŸ¯ æ‰¹é‡è§£å‹ç¼©å·¥å…·å¯åŠ¨")
        self.log_info(f"ğŸ“ å·¥ä½œç›®å½•: {self.work_dir}")
        self.log_info(f"âš™ï¸ é…ç½®é€‰é¡¹:")
        self.log_info(f"   ğŸ“‹ åˆ›å»ºå¤‡ä»½: {'æ˜¯' if self.create_backup else 'å¦'}")
        self.log_info(f"   ğŸ—‘ï¸ åˆ é™¤åŸæ–‡ä»¶: {'æ˜¯' if self.delete_original else 'å¦'}")
        self.log_info(f"   ğŸ“‚ ä¿æŒç»“æ„: {'æ˜¯' if self.preserve_structure else 'å¦'}")
        self.log_info(f"   ğŸ“„ æ‰å¹³åŒ–æå–: {'æ˜¯' if self.extract_flat else 'å¦'}")
        self.log_info(f"   ğŸ”¤ è‡ªåŠ¨å¤„ç†ä¹±ç : æ˜¯")
        
        # æ£€æŸ¥å·¥ä½œç›®å½•
        if not self.work_dir.exists():
            self.log_error(f"å·¥ä½œç›®å½•ä¸å­˜åœ¨: {self.work_dir}")
            return False
            
        if not self.work_dir.is_dir():
            self.log_error(f"æŒ‡å®šè·¯å¾„ä¸æ˜¯ç›®å½•: {self.work_dir}")
            return False
        
        # åˆ›å»ºå¤‡ä»½
        if self.create_backup:
            if not self.create_backup_copy():
                self.log_error("å¤‡ä»½åˆ›å»ºå¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
                return False
        
        # åˆå§‹æ‰«æï¼ˆä»…ç”¨äºå±•ç¤ºï¼‰
        initial_files = self.scan_compressed_files()
        if not initial_files:
            self.log_info("æ²¡æœ‰å‘ç°å‹ç¼©æ–‡ä»¶ï¼Œç¨‹åºç»“æŸ")
            return True
        
        # å¤„ç†æ–‡ä»¶ï¼ˆæ”¯æŒåµŒå¥—ï¼‰
        try:
            self.process_all_files()
            return True
        except KeyboardInterrupt:
            self.log_warning("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
            return False
        except Exception as e:
            self.log_error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ‰¹é‡è§£å‹ç¼©å·¥å…· - æ”¯æŒåµŒå¥—è§£å‹å’Œä¸­æ–‡æ–‡ä»¶åå¤„ç†',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main.py /path/to/folder                    # æ ‡å‡†æ¨¡å¼
  python main.py /path/to/folder --extract-flat     # æ‰å¹³åŒ–æå–
  python main.py /path/to/folder --no-backup        # ä¸åˆ›å»ºå¤‡ä»½
  python main.py /path/to/folder --keep-original    # ä¿ç•™åŸæ–‡ä»¶

æ”¯æŒçš„æ ¼å¼: ZIP, RAR, 7Z, TAR, TAR.GZ, TAR.BZ2, TAR.XZ
        """
    )
    
    parser.add_argument('directory', help='è¦å¤„ç†çš„ç›®å½•è·¯å¾„')
    parser.add_argument('--no-backup', action='store_true', help='ä¸åˆ›å»ºå¤‡ä»½')
    parser.add_argument('--keep-original', action='store_true', help='ä¿ç•™åŸå‹ç¼©æ–‡ä»¶')
    parser.add_argument('--flat-structure', action='store_true', help='ä¸ä¿æŒç›®å½•ç»“æ„')
    parser.add_argument('--extract-flat', action='store_true', help='æ‰å¹³åŒ–æå–ï¼ˆåªè¦æ–‡ä»¶ï¼Œä¸è¦æ–‡ä»¶å¤¹ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not Path(args.directory).exists():
        print(f"âŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨ - {args.directory}")
        sys.exit(1)
    
    # åˆ›å»ºè§£å‹å™¨å®ä¾‹
    extractor = BatchExtractor(
        work_dir=args.directory,
        create_backup=not args.no_backup,
        delete_original=not args.keep_original,
        preserve_structure=not args.flat_structure,
        extract_flat=args.extract_flat
    )
    
    # è¿è¡Œå¤„ç†
    success = extractor.run()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()